# ----------------------------
# Ana ƒ∞≈ülem Fonksiyonu
# ----------------------------
def process_file(item):
    try:
        if not isinstance(item, dict):
            raise ValueError("Ge√ßersiz √∂ƒüe formatƒ±")
        title = item.get("title")
        if not title:
            raise ValueError("Ba≈ülƒ±k bulunamadƒ±")
        dosya_path = os.path.join(STORAGE_DIR, title)
        if not os.path.exists(dosya_path):
            raise FileNotFoundError(f"Dosya bulunamadƒ±: {dosya_path}")
        if title.lower().endswith('.pdf'):
            raw_text = extract_text_from_pdf(dosya_path)
            source_type = "pdf"
        elif title.lower().endswith('.txt'):
            with open(dosya_path, 'r', encoding='utf-8') as f:
                raw_text = f.read()
            source_type = "txt"
        else:
            raise ValueError("Desteklenmeyen dosya uzantƒ±sƒ±.")
        if not raw_text:
            raise Exception("Ham metin √ßƒ±karƒ±lamadƒ±")
        
        # Haritalama: Metin √ßƒ±kartmadan √∂nce belge haritasƒ± olu≈üturuluyor
        # ƒ∞steƒüe baƒülƒ±: mapping_info loglanabilir veya sonraki i≈ülemlerde kullanƒ±labilir.
        # mapping_info = map_scientific_sections(raw_text)
        # logger.info(f"Haritalama bilgileri olu≈üturuldu: {json.dumps(mapping_info, ensure_ascii=False)}")
        
        # Tablo Olu≈üturma: Haritalama sonrasƒ±nda tablolar tespit ediliyor
        tables = detect_tables(dosya_path) if source_type == "pdf" else []
        if tables:
            tables_text = " ".join([table["baslik"] for table in tables])
        else:
            tables_text = "No table content available."
        # bu noktadan sonra yapay zeka bana bazi yordamlaronerdi bunlari inceleyecegim
        # ama once stak dosya yapisini kullanima sokmak gerekiyor.
        # Referanslar √áƒ±karma: Haritalama sonrasƒ±nda referanslar tespit ediliyor
        references = extract_references_enhanced(raw_text)
        if references:
            ref_filename = os.path.splitext(title)[0] + "_references.txt" #dosya adini kisaltarak referas dosyasini .txt uzantili yazar
            save_text_file(TEMIZ_KAYNAKCA_DIR, ref_filename, "\n".join(references))
            vosviewer_file = os.path.join(TEMIZ_KAYNAKCA_DIR, os.path.splitext(title)[0] + "_references_vosviewer.txt")
            save_text_file(TEMIZ_KAYNAKCA_DIR, vosviewer_file, "\n".join(references))
        else:
            ref_filename = "Kaynak dosyada Kullanilabilir referans bulunamadi. Kaynak dosyasi (.txt) olusturulamadi."
            vosviewer_file = "Kaynak dosyada Kullanilabilir referans bulunamadi (.vos) olusturulamadi."
        
        # Temiz Metin Olu≈üturma: Metin temizleme i≈ülemi
        temiz_metin = clean_text(raw_text)
        temiz_metin_filename = os.path.splitext(title)[0] + "_temiz_metin.txt"
        save_text_file(TEMIZ_METIN_DIR, temiz_metin_filename, temiz_metin)
        temiz_metin_vosviewer = os.path.splitext(title)[0] + "_temiz_metin_vosviewer.txt"
        save_text_file(TEMIZ_METIN_DIR, temiz_metin_vosviewer, temiz_metin)
        
        # Temiz Tablo Olu≈üturma: Tablolar temizleme i≈ülemi    
        temiz_tablo_filename = os.path.splitext(title)[0] + "_temiz_tablo.txt"
        save_text_file(TEMIZ_TABLO_DIR, temiz_tablo_filename, tables_text)
        temiz_tablo_vosviewer = os.path.splitext(title)[0] + "_temiz_tablo_vosviewer.txt"
        save_text_file(TEMIZ_TABLO_DIR, temiz_tablo_vosviewer, tables_text)
        
        # Temiz Kaynakca Olu≈üturma: Kaynakca temizleme i≈ülemi
        temiz_kaynakca_filename = os.path.splitext(title)[0] + "_temiz_kaynakca.txt"
        save_text_file(TEMIZ_KAYNAKCA_DIR, temiz_kaynakca_filename, ref_filename)
        temiz_kaynakca_vosviewer = os.path.splitext(title)[0] + "_temiz_kaynakca_vosviewer.txt"
        save_text_file(TEMIZ_KAYNAKCA_DIR, temiz_kaynakca_vosviewer, vosviewer_file)
        
        # Temiz Metin ve Tablo dosyalarƒ±nƒ±n dosya yapisini guncelleyin
        # dosya yapisini guncellemek icin kalan dosyalarin okunmasi lazim    
        dosya_adi = os.path.basename(dosya_path)    
        # LOG dosyasƒ±ndan i≈ülenen dosyayƒ± √ßƒ±kar
        # stack log dosyasi icin kalan dosyalari okumak belki main bolumune tasinmali
        kalan = kalan_dosyalari_oku()
        if dosya_adi in kalan:
            kalan.remove(dosya_adi)
        with open(LOG_DOSYASI, "w", encoding="utf-8") as f:
            f.write("\n".join(kalan) + "\n")
        
        print(f"‚úÖ ƒ∞≈ülendi: {dosya_adi}")
    except Exception as e:
        print(f"‚ùå Hata: {dosya_adi} ({e})")
        print(traceback.format_exc())
    
    print("\nüéâ T√ºm i≈ülemler ba≈üarƒ±yla tamamlandƒ±!") #silinecek veya baska yere tasinacak
       
        # onceki bolum buradan devam ediyor
        mapping_info = None
        if source_type == "pdf":
            mapping_info = map_scientific_sections_extended(raw_text)
            # ƒ∞steƒüe baƒülƒ±: mapping_info loglanabilir veya sonraki i≈ülemlerde kullanƒ±labilir.
            logger.info(f"Haritalama bilgileri olu≈üturuldu: {json.dumps(mapping_info, ensure_ascii=False)}")
        
        reflowed_text = reflow_columns(raw_text)
        tables = detect_tables(dosya_path) if source_type == "pdf" else []
        if tables:
            tables_text = " ".join([table["baslik"] for table in tables])
        else:
            tables_text = "No table content available."
        references = extract_references_enhanced(raw_text)
        if references:
            ref_filename = os.path.splitext(title)[0] + "_references.txt"
            save_text_file(TEMIZ_KAYNAKCA_DIR, ref_filename, "\n".join(references))
            vosviewer_file = os.path.join(TEMIZ_KAYNAKCA_DIR, os.path.splitext(title)[0] + "_references_vosviewer.txt")
            pajek_file = os.path.join(TEMIZ_KAYNAKCA_DIR, os.path.splitext(title)[0] + "_references_pajek.net")
            save_references_for_analysis(references, vosviewer_file, pajek_file)
        else:
            references = ["No references found."]
        temiz_metin = clean_text(reflowed_text)
        if not temiz_metin:
            temiz_metin = "No clean text extracted."
        temiz_metin_filename = os.path.splitext(title)[0] + ".temizmetin.txt"
        save_text_file(TEMIZMETIN_DIR, temiz_metin_filename, temiz_metin)
        chunks = split_text(temiz_metin, chunk_size=256)
        chunk_ids = [f"{os.path.splitext(title)[0]}_{i}" for i in range(len(chunks))]
        embeddings = []
        for idx, chunk in enumerate(chunks):
            emb = embed_text(chunk)
            if emb is None:
                logger.warning(f"Chunk {idx} i√ßin embedding olu≈üturulamadƒ±.")
                embeddings.append([0.0] * 768)
            else:
                embeddings.append(emb)
            embed_filename = f"{os.path.splitext(title)[0]}_{idx}.embed.txt"
            save_text_file(EMBEDDING_DIR, embed_filename, chunk)
        try:
            collection.add(
                ids=chunk_ids,
                embeddings=embeddings,
                metadatas=[{'title': title, 'source': source_type, 'chunk_index': i,
                            'timestamp': datetime.now().isoformat()} for i in range(len(chunks))]
            )
            logger.info(f"‚úÖ {title} i√ßin t√ºm chunk embedding'leri ChromaDB'ye eklendi.")
        except Exception as e:
            logger.error(f"Embedding eklenirken hata olu≈ütu: {e}")
        item_key = item.get("key")
        if item_key:
            bib_data = fetch_zotero_metadata(item_key)
            if bib_data:
                try:
                    bib_collection.add(
                        ids=[os.path.splitext(title)[0]],
                        embeddings=[[0.0] * 768],
                        metadatas=[{'title': title, 'bibliography': bib_data,
                                    'timestamp': datetime.now().isoformat()}]
                    )
                    logger.info(f"‚úÖ {title} i√ßin Zotero bibliyografi bilgisi eklendi.")
                    bib_str = json.dumps(bib_data)
                except Exception as e:
                    logger.error(f"Bibliyografi eklenirken hata: {e}")
                    bib_str = "No bibliographic data available."
            else:
                bib_str = "No bibliographic data available."
        else:
            bib_str = "No bibliographic data available."
        stack_guncelle(title, "i≈ülendi")
        log_entry = {
            'dosya': title,
            'tarih': datetime.now(pytz.timezone('Turkey')).isoformat(),
            'tablo_sayisi': len(tables),
            'referans_sayisi': len(references),
            'dosya_tipi': source_type,
            'bellek_kullanim': memory_usage(),
            'clean_text': temiz_metin,
            'tables': tables_text,
            'bibliography': bib_str,
            'mapping': mapping_info  # Eklenen haritalama bilgisi
        }
        return (True, log_entry)
except Exception as e:
   error_log = {
        'dosya': title if 'title' in locals() else 'unknown',
        'hata': str(e),
        'traceback': traceback.format_exc(),
        'zaman': datetime.now().isoformat()
    }
    return (False, error_log)



def main():
    global total_files, success_count, embedding_failed_count, text_extraction_failed_count
    global collection, bib_collection, cluster_results_list
    try:
        init_dirs()
        print("\n" + "="*80)
        print("### PDF/TXT ƒ∞≈ûLEME, TEMƒ∞Z METƒ∞N, EMBEDDING, ZOTERO, K√úMELEME ANALƒ∞Zƒ∞ VE HARƒ∞TLAMA Sƒ∞STEMƒ∞ ###")
        print("="*80)
        json_file_name = input("ƒ∞≈ülenecek JSON dosyasƒ±nƒ±n adƒ±nƒ± girin (√∂rneƒüin: Kitap.json): ")
        json_file_path = os.path.join(SUCCESS_DIR, json_file_name)
        if not os.path.exists(json_file_path):
            logger.error(f"‚ùå {json_file_name} dosyasƒ± bulunamadƒ±!")
            return
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        valid_items = [item for item in data if isinstance(item, dict) and item.get('title')]
        total_files = len(valid_items)
        if total_files == 0:
            logger.error("‚ùå ƒ∞≈ülenecek ge√ßerli kayƒ±t bulunamadƒ±!")
            return
        user_input = input("Ba≈ütan ba≈ülamak i√ßin [B], kaldƒ±ƒüƒ±nƒ±z yerden devam i√ßin [C], g√ºncelleme i√ßin [G]: ").lower()
        if user_input == 'b':
            logger.warning("‚ö†Ô∏è Veritabanƒ± sƒ±fƒ±rlanƒ±yor...")
            try:
                collection.delete(where={"id": {"$exists": True}})
                bib_collection.delete(where={"id": {"$exists": True}})
                collection = chroma_client.get_or_create_collection(name="pdf_embeddings")
                bib_collection = chroma_client.get_or_create_collection(name="pdf_bibliography")
            except Exception as e:
                logger.error(f"‚ùå Veritabanƒ± sƒ±fƒ±rlama hatasƒ±: {e}")
                return
            last_index = 0
        elif user_input == 'c':
            last_index = get_last_processed_index()
        else:
            last_index = 0
        print(f"\nƒ∞≈ülem ba≈ülƒ±yor... ({last_index + 1}/{total_files})")
        with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:
            futures = {executor.submit(process_file, item): item for item in valid_items[last_index:]}
            for future in tqdm(as_completed(futures), total=len(futures), desc="Dosyalar ƒ∞≈üleniyor"):
                item = futures[future]
                try:
                    success, result = future.result()
                    if success:
                        success_count += 1
                        logger.info(f"‚úÖ {item.get('title', 'Bilinmeyen dosya')} i≈ülendi")
                        cluster_results_list.append({
                            "content": result.get("clean_text", "No clean text extracted."),
                            "tables": result.get("tables", "No table content available."),
                            "bibliography": result.get("bibliography", "No bibliographic data available.")
                        })
                    else:
                        logger.error(f"‚ùå {item.get('title', 'Bilinmeyen dosya')} hatasƒ±: {result['hata']}")
                except Exception as e:
                    logger.error(f"‚ùå ƒ∞≈ülem hatasƒ±: {item.get('title', 'Bilinmeyen dosya')} - {str(e)}")
                save_last_processed_index(valid_items.index(item))
    except Exception as e:
        logger.error(f"Ana programda hata olu≈ütu: {str(e)}")
        error_log = {
            'dosya': 'main',
            'hata': str(e),
            'traceback': traceback.format_exc(),
            'zaman': datetime.now().isoformat()
        }
        logger.error(error_log)
        traceback.print_exc()
    finally:
        print("\n" + "="*60)
        print(f"ƒ∞≈ülem tamamlandƒ±!")
        print(f"Toplam dosya: {total_files}")
        print(f"Ba≈üarƒ±lƒ±: {success_count}")
        print(f"Embedding hatasƒ±: {embedding_failed_count}")
        print(f"Metin √ßƒ±karma hatasƒ±: {text_extraction_failed_count}")
        print("="*60)
 
        if not cluster_results_list:
    try:
        df = pd.read_csv("fine_tuning_dataset.csv", encoding='utf-8')
        cluster_results_list = df.to_dict(orient="records")
        logger.info("√ñnceki fine-tuning veri seti y√ºklendi")
    except FileNotFoundError:
        logger.warning("√ñnceki fine-tuning veri seti bulunamadƒ±")
    except Exception as e:
        logger.error(f"Veri seti okuma hatasƒ±: {str(e)}")
        cluster_results_list = []
if cluster_results_list:
    try:
        cluster_analysis_from_chromadb(cluster_results_list, n_clusters=5, output_dir="cluster_results")
        logger.info("K√ºmeleme analizi ba≈üarƒ±yla tamamlandƒ±")
    except Exception as e:
        logger.error(f"K√ºmeleme analizi hatasƒ±: {str(e)}")
        # Ek √∂zellikler GUI'sini ba≈ülat
try:
    app = AdditionalFeaturesGUI()
    app.mainloop()
except Exception as e:
    logger.error(f"GUI ba≈ülatma hatasƒ±: {str(e)}")
    print("GUI ba≈ülatƒ±lamadƒ±, program sonlandƒ±rƒ±lƒ±yor...")


if __name__ == '__main__':
    multiprocessing.freeze_support()
    main()

